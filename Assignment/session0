Questions:
1. What are Channels and Kernels (according to EVA)?

2. Why should we (nearly) always use 3x3 kernels?

3. How many times do we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)

4. How are kernels initialized?

5. What happens during the training of a DNN?

Answers:
1. What are Channels and Kernels (according to EVA)?
Kernels :
Suppose you have an image and you want to extract some useful features out of it, that is when you will need kernels.
Kernel is a feature extractor.
It is a tool or matrix ( generally 3*3) that convolves over an image(matrix) or feature map to extract one feature.
It extracts some information from the given image and the information is stored in Neuron.
Kernel creates channels from an image (Collection of neurons is a channel).
Eg: A vertical edge detector kernel will extract all vertical edges from an image and the collection of all vertical edges in that image is a channel.
Each kernel makes its channel.
Channels :
A channel is a collection of single-type features extracted by kernels.
One channel is like a container, containing a similar kind of information.
Channels can be seen, but kernels cannot be seen.
Information in channels depends on the type of kernel/kernel values used to extract that information.
At the first level, one channel contains a single feature.
Through subsequent layers, one channel can contain combined low-level information or features extracted from the previous layers as well.
So after a few layers of convolution network, if we combine features from previous layers and see them as a single feature, that can be further contained with one channel to learn that particular feature.
2. Why should we (nearly) always use 3x3 kernels?
Smaller kernels result in a fewer number of parameters.

Eg: Let us consider a 5x5 image and if we want to extract a feature that covers all the pixels of the image or say whose receptive field is the entire image.
Then we can either use a 5x5 kernel which is 5x5 = 25 parameters or we could use 2 layers of 3x3 kernels.
1st kernel extracts a 3x3 channel from the image and 2nd kernel extracts a single feature from that channel, thus the receptive field becomes the whole image.
But in this case, we have 2 layers of 3x3 kernels which are 233 = 18 parameters.
As the image size increases, the difference between the number of parameters also increases drastically.
Smaller kernels = More features.

Assume we have a 7x7 image and we are trying to extract features out of it.
If we use a 7x7 kernel, then we get only 1 feature out of the image.
If we use a 5x5 kernel, then we get a 3x3 channel of some feature depending on kernel values, thus 9 features in total.
If we use a 3x3 kernel, then we get 5x5 channels and thus 25 features.
The more features we have, the better model training will be, and also our predictions will be better.
Now a question arises, if smaller kernels are the best option, then why not a 2x2 or 1x1 kernel?

We should always use odd-numbered kernels, reason will be revealed soon in upcoming sessions.
In the case of 1x1 kernels, all you will be doing is just copy-paste the pixel, without extracting any information from it.
3. How many times do we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)
It takes 99 convolution operations to reach from 199x199 to 1x1.
199x199 | 3x3 > 197x197
197x197 | 3x3 > 195x195
195x195 | 3x3 > 193x193
193x193 | 3x3 > 191x191
191x191 | 3x3 > 189x189
189x189 | 3x3 > 187x187
187x187 | 3x3 > 185x185
185x185 | 3x3 > 183x183
183x183 | 3x3 > 181x181
181x181 | 3x3 > 179x179
179x179 | 3x3 > 177x177
177x177 | 3x3 > 175x175
175x175 | 3x3 > 173x173
173x173 | 3x3 > 171x171
171x171 | 3x3 > 169x169
169x169 | 3x3 > 167x167
167x167 | 3x3 > 165x165
165x165 | 3x3 > 163x163
163x163 | 3x3 > 161x161
161x161 | 3x3 > 159x159
159x159 | 3x3 > 157x157
157x157 | 3x3 > 155x155
155x155 | 3x3 > 153x153
153x153 | 3x3 > 151x151
151x151 | 3x3 > 149x149
149x149 | 3x3 > 147x147
147x147 | 3x3 > 145x145
145x145 | 3x3 > 143x143
143x143 | 3x3 > 141x141
141x141 | 3x3 > 139x139
139x139 | 3x3 > 137x137
137x137 | 3x3 > 135x135
135x135 | 3x3 > 133x133
133x133 | 3x3 > 131x131
131x131 | 3x3 > 129x129
129x129 | 3x3 > 127x127
127x127 | 3x3 > 125x125
125x125 | 3x3 > 123x123
123x123 | 3x3 > 121x121
121x121 | 3x3 > 119x119
119x119 | 3x3 > 117x117
117x117 | 3x3 > 115x115
115x115 | 3x3 > 113x113
113x113 | 3x3 > 111x111
111x111 | 3x3 > 109x109
109x109 | 3x3 > 107x107
107x107 | 3x3 > 105x105
105x105 | 3x3 > 103x103
103x103 | 3x3 > 101x101
101x101 | 3x3 > 99x99
  99x99 | 3x3 > 97x97
  97x97 | 3x3 > 95x95
  95x95 | 3x3 > 93x93
  93x93 | 3x3 > 91x91
  91x91 | 3x3 > 89x89
  89x89 | 3x3 > 87x87
  87x87 | 3x3 > 85x85
  85x85 | 3x3 > 83x83
  83x83 | 3x3 > 81x81
  81x81 | 3x3 > 79x79
  79x79 | 3x3 > 77x77
  77x77 | 3x3 > 75x75
  75x75 | 3x3 > 73x73
  73x73 | 3x3 > 71x71
  71x71 | 3x3 > 69x69
  69x69 | 3x3 > 67x67
  67x67 | 3x3 > 65x65
  65x65 | 3x3 > 63x63
  63x63 | 3x3 > 61x61
  61x61 | 3x3 > 59x59
  59x59 | 3x3 > 57x57
  57x57 | 3x3 > 55x55
  55x55 | 3x3 > 53x53
  53x53 | 3x3 > 51x51
  51x51 | 3x3 > 49x49
  49x49 | 3x3 > 47x47
  47x47 | 3x3 > 45x45
  45x45 | 3x3 > 43x43
  43x43 | 3x3 > 41x41
  41x41 | 3x3 > 39x39
  39x39 | 3x3 > 37x37
  37x37 | 3x3 > 35x35
  35x35 | 3x3 > 33x33
  33x33 | 3x3 > 31x31
  31x31 | 3x3 > 29x29
  29x29 | 3x3 > 27x27
  27x27 | 3x3 > 25x25
  25x25 | 3x3 > 23x23
  23x23 | 3x3 > 21x21
  21x21 | 3x3 > 19x19
  19x19 | 3x3 > 17x17
  17x17 | 3x3 > 15x15
  15x15 | 3x3 > 13x13
  13x13 | 3x3 > 11x11
  11x11 | 3x3 > 9x9
    9x9 | 3x3 > 7x7
    7x7 | 3x3 > 5x5
    5x5 | 3x3 > 3x3
    3x3 | 3x3 > 1x1
4. How are kernels initialized?
Kernels are initialized by random values.

Reason:

We don't exactly know which features a model is going to extract in a particular layer, so we just initialize random values and let the model change the values such that the kernel extracts features required by our model.
Also, we don't want our model to be biassed towards any particular feature if we manually initialize the values.
But, let us consider the following scenario,

Input - Image
Output - All vertical Edges in that Image.
In such a case, instead of assigning random values to the kernel, we can initialize the kernel to standard edge detector values like
1 | 0 | -1
2 | 0 | -2
1 | 0 | -1
But, if our domain is only Neural Networks, then kernel values are initialized randomly.
5. What happens during the training of a DNN?
Structure of DNN:

Deep Neural network comprises one input layer, one output layer, and multiple hidden layers.
Each layer has got multiple neurons that store some values or numbers or activations (between 0 and 1).
We can also have some activation function assigned to some or all neurons in layers.
The activation function modifies the value of neurons while training.
Neurons in one layer are influenced by the neurons of the previous layers or layers on the top. There is a weighted sum taken from the neurons in the previous layer and an activation function is applied to keep the value of the weighted sum in the acceptable range. So, during the forward pass in the network, the activation function keeps doing the transformation on input received from the previous layers.
Training DNN:

The way learning happens in the DNN is through a backward pass.
When we provide input to the network, it predicts the forward pass. The prediction might be wrong or correct.
The network is provided with this information feedback through backpropagation.
In the back pass, the network is provided with the feedback to change it according to the actual ground truth in this supervised learning process. So the network learns by looking at the actual output and updates the weight inside the network according to an algorithm called backpropagation.
This method calculates the error function and takes the gradient of that (slope) concerning the weights and updates its weight according to that. The process of providing the input and updating the weights in this way is repeated several times with more and more data. This helps the network update the weights such that when predicting on unseen data, the weights learned or the trained model gives the correct predictions.
According to EVA:

During the training of a DNN, the base layer neurons store all the pixels of image/input.
In each layer, there is a kernel that extracts features or combines low-level features to give high-level features.
Kernel values are initialized randomly, then, during the training, the model learns and corrects these values based on the features it intends to extract.
1st layer combines pixels to detect edges
2nd layer combines edges to make textures, gradients
3rd layer combines textures to make patterns
4th layer combines patterns to make parts of objects
5th layer combines parts of objects to make objects
Finally, the name/type of object is predicted
